from TweetAnalyzer import TweetAnalyzer
import TweetPurgeLib as tpl
import csv
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report
from sklearn.linear_model import SGDClassifier
import GetData
from EvalFileBuilder import Builder
import nltk
from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.grid_search import GridSearchCV
from sklearn import svm
from pprint import pprint
from time import time
from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(n_estimators=10, oob_score=True, n_jobs=1) 
forest.fit([[1,2,3],[1,4,5],[0,3,2],[5,3,5]],[0,1,0,1]).predict([[2,3,5]])
array([1])
forest = RandomForestClassifier(n_estimators='10', oob_score=True, n_jobs=1)
forest.fit([[1,2,3],[1,4,5],[0,3,2],[5,3,5]],[0,1,0,1]).predict([2,3,5])    Traceback (most recent call last):
if __name__ == "__main__":

    classes = ['positive', 'rest_world']

    train_data = []
    train_labels = []
    test_data = []
    test_labels = []
    test_id = []

    stemmer = nltk.stem.snowball.ItalianStemmer(ignore_stopwords=True)

    train_data, train_labels, test_data, test_labels, test_id = GetData.getPosVsWorld("sentipolc annotation gold v2.csv")

    text_clfPos = Pipeline([('vect', CountVectorizer()),
                            ('tfidf', TfidfTransformer(use_idf=True)),
                            ('clf', svm.LinearSVC()),
                            ])

    parameters = {
        'vect__analyzer': (TweetAnalyzer(None, stemmer), 'word'),
        'vect__max_df': (0.5, 0.75, 1.0),
        'vect__max_features': (None, 5000, 10000, 50000),
        'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams
        'tfidf__use_idf': (True, False),
        'tfidf__norm': ('l1', 'l2'),
        'clf__dual': (True, False),
        'clf__class_weight': ('balanced', {'positive': 5, 'rest_world': 1})
    }

    grid_search = GridSearchCV(text_clfPos, parameters, n_jobs=-1, verbose=1)

    print("Performing grid search...")
    print("pipeline:", [name for name, _ in text_clfPos.steps])
    print("parameters:")
    pprint(parameters)
    t0 = time()
    grid_search.fit(train_data, train_labels)
    print("done in %0.3fs" % (time() - t0))
    print()

    print("Best score: %0.3f" % grid_search.best_score_)
    print("Best parameters set:")
    best_parameters = grid_search.best_estimator_.get_params()
    for param_name in sorted(parameters.keys()):
        print("\t%s: %r" % (param_name, best_parameters[param_name]))